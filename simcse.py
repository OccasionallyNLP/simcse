# -*- coding: utf-8 -*-
"""simcse.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14J16R3FOuGq_3l8LLIQlmFnTepH82jtz
"""

from google.colab import drive
drive.mount('/content/gdrive')

! pip install sentencepiece
! pip install transformers

import pandas as pd
from tqdm import tqdm
import json
import seaborn as sns 
from collections import defaultdict
import random
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import tensor as T
import transformers
from transformers import AutoTokenizer, RobertaModel, RobertaConfig, BertConfig, BertModel
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset
import numpy as np
from scipy.stats import spearmanr
import os
import argparse
import logging
from torch.cuda.amp import autocast
from torch.cuda.amp import GradScaler
import copy
from matplotlib import pyplot as plt

# -*- coding: utf-8 -*-
# data utils 
import json
import os
from tqdm import tqdm
import argparse

def str2bool(v):
    """
    Transform user input(argument) to be boolean expression.
    :param v: (string) user input
    :return: Bool(True, False)
    """
    if v.lower() in ("yes", "true", "t", "y", "1"):
        return True
    elif v.lower() in ("no", "false", "f", "n", "0"):
        return False
    else:
        raise argparse.ArgumentTypeError("Boolean value expected.")

def save_jsonl(address,data,name):
    f = open(os.path.join(address,name+'.jsonl'),'w',encoding = 'utf-8')
    for i in data:
        f.write(json.dumps(i)+'\n')
        
def load_jsonl(path):
    result = []
    f = open(path,'r',encoding = 'utf-8')
    for i in tqdm(f):
        result.append(json.loads(i))
    return result

cd /content/gdrive/MyDrive/data

# parser
parser = argparse.ArgumentParser()

# data
parser.add_argument('--train_data', type=str,default =  r'./train_add.csv')
parser.add_argument('--val_data', type=str,default =  r'./sts-dev.tsv')
parser.add_argument('--output_dir', type=str, default = './output/simcse/bert')

# logging 관련
parser.add_argument('--logging_term', type=int, default = 250)

# 학습 관련
parser.add_argument('--epochs', type=int, default = 3)
parser.add_argument('--batch_size', type=int, default = 128)
parser.add_argument('--lr', type=float, default = 5e-5)
parser.add_argument('--warmup', type=float, default = 0.05)
parser.add_argument('--eval_step', type=int, default = 250)
parser.add_argument('--fp16', type=str2bool, default = True)
parser.add_argument('--decay', type=float, default = 0.05)
parser.add_argument('--temp', type=float, default = 0.05)

# 데이터 관련
parser.add_argument('--max_length',type= int, default = 50)

args, _ = parser.parse_known_args()

# model, tokenizer
roberta = RobertaModel.from_pretrained("klue/roberta-base", add_pooling_layer = False)
tokenizer = AutoTokenizer.from_pretrained("klue/roberta-base")
config = RobertaConfig.from_pretrained('klue/roberta-base')

# model, tokenizer
bert = BertModel.from_pretrained("klue/bert-base", add_pooling_layer = False)
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
config = BertConfig.from_pretrained('klue/bert-base')

"""# Data - Organize,Prepare

## korNLI, korSTS
"""

f = open(args.val_data,encoding='utf-8')
data = []
for i in f.readlines():
    data.append([j.strip() for j in i.split('\t')])
val_data = pd.DataFrame(data[1:],columns=data[0])
val_data['score']=val_data['score'].astype('float')/5.
val_data = val_data.loc[:,['sentence1','sentence2','score']]
val_data_ = []
for i in tqdm(val_data.index):
    val_data_.append(dict(sentence1=val_data.loc[i,'sentence1'],sentence2=val_data.loc[i,'sentence2'], label=val_data.loc[i,'score']))

train_data = pd.read_csv(args.train_data)

# 동일한 premise에서 하나의 hard negative sampling
premise_dict = defaultdict(set)
for i in tqdm(train_data.index):
    if train_data.loc[i,'label']==2: # contradiction
        premise_dict[train_data.loc[i,'premise']].add(train_data.loc[i,'hypothesis'])    
train_data_ = []
for i in tqdm(train_data.index):
    if train_data.loc[i,'label']==1: # entailment
        tmp = train_data.loc[i,'premise']
        if premise_dict.get(tmp, -1)!=-1:
            train_data_.append(dict(anchor=train_data.loc[i,'premise'],positive=train_data.loc[i,'hypothesis'], negative=random.sample(premise_dict[tmp],k=1)[0])) 
print(f'{len(train_data)}----->{len(train_data_)}')

"""# KLUE-NLI,STS

# Dataset
"""

# -*- coding: utf-8 -*-
# data utils 로 빼두기.
import json
import os
from tqdm import tqdm
import numpy as np
import torch
from torch.utils.data import Dataset
from torch import tensor as T
from typing import List
import pandas as pd

class NLI_Dataset(Dataset):
    def __init__(self, data:List[dict], tokenizer, max_length : int):
        super().__init__()
        self.data = data
        self.tokenizer = tokenizer 
        self.max_length = max_length

    def get_feature(self,index):
        i = self.data[index]
        output1=self.tensorize(sentence1=i['anchor'], sentence2=None, tokenizer = self.tokenizer, max_length=self.max_length)
        output2=self.tensorize(sentence1=i['positive'], sentence2=None, tokenizer = self.tokenizer, max_length=self.max_length)
        output3=self.tensorize(sentence1=i['negative'], sentence2=None, tokenizer = self.tokenizer, max_length=self.max_length)
        return {'input_ids':T([output1[0],output2[0],output3[0]]), 'attention_mask':T([output1[1],output2[1],output3[1]]), 'token_type_ids':T([output1[2],output2[2],output3[2]])}
        
    @staticmethod
    def tensorize(sentence1:str, sentence2:str, tokenizer, max_length:int):
        tokenized1 = tokenizer.tokenize(sentence1)
        ids1 = tokenizer.convert_tokens_to_ids(tokenized1)
        if sentence2 is None:
            # trunc
            ids1 = ids1[:(max_length-2)] # start token, end token
            ids1 = [tokenizer.cls_token_id]+ids1+[tokenizer.sep_token_id]
            attention_mask1 =[1]*len(ids1)+[0]*(max_length - len(ids1))
            token_type_id1=[0]*max_length
            input_ids1 = ids1 + [tokenizer.pad_token_id]*(max_length - len(ids1))
            return input_ids1, attention_mask1, token_type_id1
        else:
            tokenized2 = tokenizer.tokenize(sentence2)
            ids2 = tokenizer.convert_tokens_to_ids(tokenized2)
            # title과 context를 더하기 위함임
            ids = ids1 +[tokenizer.sep_token_id]+ids2
            ids = ids[:(max_length-2)]
            ids = [tokenizer.cls_token_id]+ids+[tokenizer.sep_token_id]
            attention_mask =[1]*len(ids)+[0]*(max_length - len(ids))
            token_type_ids = [0]*(len(ids1)+2)+[1]*(max_length - len(ids1)-2)
            input_ids = ids + [tokenizer.pad_token_id]*(max_length - len(ids))

            return input_ids, attention_mask,token_type_ids
            
            
    def __getitem__(self, index):
        return self.get_feature(index)
    
    def __len__(self):
        return len(self.data)

class STS_Dataset(Dataset):
    def __init__(self, data:List[dict], tokenizer, max_length : int):
        super().__init__()
        self.data = data
        self.tokenizer = tokenizer 
        self.max_length = max_length

    def get_feature(self,index):
        i = self.data[index]
        output1=self.tensorize(sentence1=i['sentence1'], sentence2=None,tokenizer = self.tokenizer, max_length=self.max_length)
        output2=self.tensorize(sentence1=i['sentence2'], sentence2=None,tokenizer = self.tokenizer, max_length=self.max_length)
        if i.get('label',-1)!=-1:
            return {'input_ids':T([output1[0],output2[0]]), 'attention_mask':T([output1[1],output2[1]]), 'token_type_ids':T([output1[2],output2[2]]), 'labels':i['label']}
        else:
            return {'input_ids':T([output1[0],output2[0]]), 'attention_mask':T([output1[1],output2[1]]), 'token_type_ids':T([output1[2],output2[2]])}
        
    @staticmethod
    def tensorize(sentence1:str, sentence2:str, tokenizer, max_length:int):
        tokenized1 = tokenizer.tokenize(sentence1)
        ids1 = tokenizer.convert_tokens_to_ids(tokenized1)
        if sentence2 is None:
            # trunc
            ids1 = ids1[:(max_length-2)] # start token, end token
            ids1 = [tokenizer.cls_token_id]+ids1+[tokenizer.sep_token_id]
            attention_mask1 =[1]*len(ids1)+[0]*(max_length - len(ids1))
            token_type_id1=[0]*max_length
            input_ids1 = ids1 + [tokenizer.pad_token_id]*(max_length - len(ids1))
            return input_ids1, attention_mask1, token_type_id1
        else:
            tokenized2 = tokenizer.tokenize(sentence2)
            ids2 = tokenizer.convert_tokens_to_ids(tokenized2)
            # title과 context를 더하기 위함임
            ids = ids1 +[tokenizer.sep_token_id]+ids2
            ids = ids[:(max_length-2)]
            ids = [tokenizer.cls_token_id]+ids+[tokenizer.sep_token_id]
            attention_mask =[1]*len(ids)+[0]*(max_length - len(ids))
            token_type_ids = [0]*(len(ids1)+2)+[1]*(max_length - len(ids1)-2)
            input_ids = ids + [tokenizer.pad_token_id]*(max_length - len(ids))

            return input_ids, attention_mask,token_type_ids
            
            
    def __getitem__(self, index):
        return self.get_feature(index)
    
    def __len__(self):
        return len(self.data)

train_dataset = NLI_Dataset(data=train_data_,tokenizer=tokenizer, max_length=args.max_length)
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset,batch_size = args.batch_size, sampler = train_sampler)
val_dataset = STS_Dataset(data=val_data_,tokenizer=tokenizer, max_length=args.max_length)
val_sampler = SequentialSampler(val_dataset)
val_dataloader = DataLoader(val_dataset,batch_size = args.batch_size, sampler = val_sampler)

"""## SentenceRobertaModel"""

import torch
import torch.nn.functional as F
import torch.nn as nn
from torch import tensor as T
from transformers import PreTrainedModel,RobertaModel, BertModel
class Similarity(nn.Module):
    """
    Dot product or cosine similarity
    """

    def __init__(self, temp):
        super().__init__()
        self.temp = temp
        self.cos = nn.CosineSimilarity(dim=-1)

    def forward(self, x, y):
        return self.cos(x, y) / self.temp

class SentenceRobertaModel(PreTrainedModel):
    def __init__(self, config, args, model):
        super().__init__(config)
        self.args = args
        self.pretrainedModel = model(config, add_pooling_layer = False)
        self.pooler = nn.Sequential(nn.Linear(config.hidden_size,config.hidden_size),nn.Tanh())
        self._init_weights(self.pooler)
        self.cos_sim = Similarity(self.args.temp)
    def init_pretrained_model(self, state_dict):
        # init pretrained model
        self.pretrainedModel.load_state_dict(state_dict) 
    
    def _init_weights(self, modulelist):
        """Initialize the weights"""
        for module in modulelist:
            if isinstance(module, nn.Linear):
            # Slightly different from the TF version which uses truncated_normal for initialization
            # cf https://github.com/pytorch/pytorch/pull/5617
                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
                if module.bias is not None:
                    module.bias.data.zero_()  
    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):
        # input_ids : (bs, num_sent, seq_len)
        bs, num_sent, seq_len = input_ids.shape
        input_ids = input_ids.reshape(-1, seq_len) # bs*num_sent, seq_len
        attention_mask = attention_mask.reshape(-1, seq_len)
        token_type_ids = token_type_ids.reshape(-1, seq_len)
        # out : (bs*num_sent, seq_len, dim)
        out = self.pretrainedModel.forward(input_ids, attention_mask, token_type_ids)
        out = out[0][:,0,:] # bs*num_sent, dim
        output = self.pooler.forward(out)
        output = output.reshape(bs, num_sent, -1) # bs, num_sent, dim
        if num_sent == 2:
            z1,z2 = output[:,0],output[:,1] # sentence, positive
            return z1,z2
        elif num_sent == 3:
            z1,z2,z3 = output[:,0],output[:,1],output[:,2] # sentence, positive, negative
            if labels is not None:
                c1 = self.cos_sim(z1.unsqueeze(1),z2.unsqueeze(0)) # bs, bs
                c2 = self.cos_sim(z1.unsqueeze(1),z3.unsqueeze(0)) # bs, bs
                c =  torch.cat([c1,c2],dim=1) # bs, 2*bs
                return F.cross_entropy(c,labels)
            else:
                return z1,z2,z3

def get_log(args):
    global logger1, logger2
    logger1 = logging.getLogger('file') # 적지 않으면 root로 생성
    logger2 = logging.getLogger('stream') # 적지 않으면 root로 생성

    # 2. logging level 지정 - 기본 level Warning
    logger1.setLevel(logging.INFO)
    logger2.setLevel(logging.INFO)

    # 3. logging formatting 설정 - 문자열 format과 유사 - 시간, logging 이름, level - messages
    formatter = logging.Formatter('[%(asctime)s][%(name)s][%(levelname)s] >> %(message)s')

    # 4. handler : log message를 지정된 대상으로 전달하는 역할.
    # SteamHandler : steam(terminal 같은 console 창)에 log message를 보냄
    # FileHandler : 특정 file에 log message를 보내 저장시킴.
    # handler 정의
    stream_handler = logging.StreamHandler()
    # handler에 format 지정
    stream_handler.setFormatter(formatter)
    # logger instance에 handler 삽입
    logger2.addHandler(stream_handler)
    os.makedirs(args.output_dir,exist_ok=True)
    file_handler = logging.FileHandler(os.path.join(args.output_dir,'train_log.txt'), encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger1.addHandler(file_handler)

get_log(args)
logger1.info(args)
logger2.info(args)

# model = SentenceRobertaModel(config,args,RobertaModel)#BertModel
model = SentenceRobertaModel(config,args,BertModel)
#model.init_pretrained_model(roberta.state_dict())
model.init_pretrained_model(bert.state_dict())
if torch.cuda.is_available():
    model.cuda()
optimizer = torch.optim.AdamW(model.parameters(),args.lr,weight_decay=args.decay)    
total_step = len(train_dataloader)*args.epochs
warmup = total_step * args.warmup
#scheduler = transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=int(warmup), num_training_steps=total_step, num_cycles=args.cycle_term)
scheduler = transformers.get_constant_schedule_with_warmup(optimizer = optimizer, num_warmup_steps=warmup)
if args.fp16:
    scaler = GradScaler()

def eval(model, dataloader):
    model.eval()
    Sentence1=[]
    Sentence2=[]
    Actual=[]
    with torch.no_grad():
        for data in dataloader:
            if torch.cuda.is_available():
                data = {i:j.cuda() for i,j in data.items()}
            s1,s2 = model.forward(**data)
            
            Sentence1.append(s1)
            Sentence2.append(s2)
            Actual.extend(data['labels'].cpu().tolist())
    Predict = F.cosine_similarity(torch.cat(Sentence1,0), torch.cat(Sentence2,0))
    cor, pvalue = spearmanr(Actual, Predict.cpu().tolist())
    return cor

best_score = -float('inf')
best_model = None
losss= []
global_step = 0

for epoch in range(1, args.epochs+1):
    Loss = 0.
    c = 0
    iter_bar = tqdm(train_dataloader, desc='train_step')
    for data in iter_bar:
        model.train()
        optimizer.zero_grad()
        if torch.cuda.is_available():
            data = {i:j.cuda() for i,j in data.items()}
        labels = torch.arange(data['input_ids'].size(0),device = 'cuda' if torch.cuda.is_available else 'cpu')#*2
        data['labels']=labels
        if args.fp16:
            with autocast():
                loss = model.forward(**data)
                scaler.scale(loss).backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
                
        else:
            loss = model.forward(**data)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=2.0)
            optimizer.step()
        global_step+=1           
        c+=1
        scheduler.step()
        
        Loss+=loss.item()
        iter_bar.set_postfix({'epoch':epoch, 'global_step':global_step, 'lr':f"{scheduler.get_last_lr()[0]:.6f}", 'epoch_loss':f'{Loss/c:.5f}'})
        if global_step%args.logging_term == 0:
            logger2.info({'epoch':epoch, 'global_step':global_step, 'lr':f"{scheduler.get_last_lr()[0]:.6f}", 'epoch_loss':f'{Loss/c:.5f}'})
        # validation
        if global_step%args.eval_step==0:
            cor=eval(model, val_dataloader)
            logger1.info(f'step : {global_step} ----- Val_cor : {cor:.3f}')
            logger2.info(f'step : {global_step} ----- Val_cor : {cor:.3f}')    
            if cor>best_score:
                best_score = cor
                best_model = copy.deepcopy(model.state_dict())
                torch.save(best_model,os.path.join(args.output_dir,'best_model'))
    cor=eval(model, val_dataloader)
    logger1.info(f'step : {global_step} ----- Val_cor : {cor:.3f}')
    logger2.info(f'step : {global_step} ----- Val_cor : {cor:.3f}')    
    if cor>best_score:
        best_score = cor
        best_model = copy.deepcopy(model.state_dict())
        torch.save(best_model,os.path.join(args.output_dir,'best_model'))
    
    logger1.info(f'epoch : {epoch} ----- Train_Loss : {Loss/len(train_dataloader):.3f}')
    logger2.info(f'epoch : {epoch} ----- Train_Loss : {Loss/len(train_dataloader):.3f}')
    losss.append(Loss/len(train_dataloader))
    # 저장시 - gpu 0번 것만 저장 - barrier 필수 
    # torch.distributed.barrier()
logger1.info('train_end')
logger2.info('train end')

from matplotlib import pyplot as plt
plt.plot(loss)